{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session(name=\"Structured API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "\n",
    "As chema defines\n",
    "- Colume names \n",
    "- Colume types\n",
    "\n",
    "Ways to define schema\n",
    "- Define manually\n",
    "- schema on read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Spark Types\n",
    "\n",
    "- Spark internal engine `Catalyst`: maintains its ownt type information\n",
    "- Spark run codes using `Spark types`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset vs. Dataframe\n",
    "\n",
    "- Dataset: check types at `compile time`\n",
    "- Dataframe: check types at `runtime`\n",
    "\n",
    "*Dataset is Only to JVM language*: in Scala -> DataFrames = Dataset[Row]\n",
    "\n",
    "`Row`: Spark's internal optimized format.\n",
    "\n",
    "More for Catalyst: \n",
    "- https://www.youtube.com/watch?v=5ajs8EIPWGI&feature=youtu.be\n",
    "- https://www.youtube.com/watch?v=GDeePbbCz2g&feature=youtu.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns \n",
    "\n",
    "`Column` Type: it can represent *simple type* and *complex type*\n",
    "\n",
    "### Rows\n",
    "`Row` Type: a record of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark types\n",
    "\n",
    "A full list of Spark Types binding in each language: https://spark.apache.org/docs/latest/sql-reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteType"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "b = ByteType()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Structured API Execution\n",
    "\n",
    "1. Write DataFrame/Dataset/SQL Code.\n",
    "2. If valid code, Spark converts this to a `Logical Plan`.\n",
    "3. Spark transforms this Logical Plan to a `Physical Plan`, checking for optimizations along\n",
    "the way.\n",
    "4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.\n",
    "\n",
    "User code are sent to `Catalyst Optimizer`, then convert to logical plan & physical plan.\n",
    "\n",
    "More details about logical and physical planning:   \n",
    "**Spark the definitive guide: p63** Important graph over there ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
